{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_elements(browser, url):\n",
    "    \"\"\"Gets all href elements from a given url page using the browser webdriver\"\"\"\n",
    "    # Load page\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  # Wait a bit for the page to load\n",
    "    # Get page HTML\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML, 'lxml')\n",
    "    # Find all elements with the desired href attribute and add them to a list\n",
    "    elements = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^/detail/\")})]\n",
    "    return elements[::2]  # Return every other element\n",
    "\n",
    "def get_max_pages(soup):\n",
    "    \"\"\"Gets the maximum number of pages from a BeautifulSoup object\"\"\"\n",
    "    # Find the text showing the total number of records\n",
    "    records = soup.find_all(class_='numero ng-binding')[1].text\n",
    "    # Extract the number from the text\n",
    "    records = int(\"\".join(re.split(r'\\D', records)))\n",
    "    # Calculate the number of pages (20 records per page)\n",
    "    return math.ceil(records / 20)\n",
    "\n",
    "def print_scraping_info(typ_obchodu, typ_stavby, records, max_page, pages):\n",
    "    \"\"\"Prints some information about what is being scraped\"\"\"\n",
    "    print(\"----------------\")\n",
    "    print(\"Scraping: \" + typ_obchodu + \" \" + typ_stavby)\n",
    "    print(\"Total listings: \" + str(records))\n",
    "    print(\"Total pages: \" + str(max_page))\n",
    "    print(\"Scraping (only) \" + str(pages) + \" pages.\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "def scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-minimized\") # Starts Chrome minimized\n",
    "    \"\"\"Scrape a given number of pages for href elements\"\"\"\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    # Get elements from the first page\n",
    "    elements = get_page_elements(browser, url)\n",
    "    # Print information about what we're scraping\n",
    "    print_scraping_info(typ_obchodu, typ_stavby, len(elements), max_page, pages)\n",
    "    # Scrape the remaining pages\n",
    "    for i in range(1, pages):\n",
    "        # Calculate elapsed time and remaining time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        time_per_page = elapsed_time / i\n",
    "        remaining_pages = pages - i\n",
    "        remaining_time_estimate = time_per_page * remaining_pages\n",
    "\n",
    "        print('\\r'+\"Page \" + str(i) + \" = \" + str(round(100*i/pages, 2)) + \"% progress. Estimated remaining time: \" + str(round(remaining_time_estimate, 2)) + \" seconds.\", end=\"\")\n",
    "\n",
    "        new_url = url + \"?strana=\" + str(i+1)\n",
    "        new_elements = get_page_elements(browser, new_url)\n",
    "        elements.extend(new_elements)\n",
    "    browser.quit()\n",
    "    return elements\n",
    "\n",
    "def get_id(url):\n",
    "    \"\"\"Extract the ID from a URL\"\"\"\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "def elements_and_ids(x):\n",
    "    \"\"\"Create a DataFrame of URLs and IDs, remove duplicates, and save to a CSV file\"\"\"\n",
    "    elements = pd.DataFrame({\"url\":x})\n",
    "    elements[\"url_id\"] = elements[\"url\"].apply(get_id)\n",
    "    \n",
    "    len_before = len(elements)\n",
    "    # Remove duplicates\n",
    "    elements.drop_duplicates(subset=[\"url\", \"url_id\"], keep=\"first\", inplace=True)\n",
    "    len_after = len(elements)\n",
    "    \n",
    "    print(f\"-- Removed {len_before - len_after} records due to duplication.\")\n",
    "    today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    filename = f'{today_date}_urls.csv'\n",
    "    elements.to_csv(filename, index=False)\n",
    "    return elements\n",
    "\n",
    "def get_soup_elements(typ_obchodu=\"prodej\", typ_stavby=\"byty\", pages=1):\n",
    "    \"\"\"Main function to get href elements for a given type of trade and construction\"\"\"\n",
    "    url = f\"https://www.sreality.cz/hledani/{typ_obchodu}/{typ_stavby}\"\n",
    "    browser = webdriver.Chrome()\n",
    "    # Get initial soup to extract maximum pages\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  \n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml')\n",
    "    max_page = get_max_pages(soup)\n",
    "    # If 2000 pages were requested, set pages to max_page\n",
    "    if pages == 2000:\n",
    "        pages = max_page\n",
    "    # Start scraping pages\n",
    "    elements = scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages)\n",
    "    # Pass the elements to the elements_and_ids function\n",
    "    elements_df = elements_and_ids(elements)\n",
    "    return elements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Scraping: prodej byty\n",
      "Total listings: 70\n",
      "Total pages: 1008\n",
      "Scraping (only) 1008 pages.\n",
      "----------------\n",
      "Page 30 = 2.98% progress. Estimated remaining time: 1696.47 seconds."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m get_soup_elements(typ_obchodu \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mprodej\u001b[39;49m\u001b[39m\"\u001b[39;49m, typ_stavby \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mbyty\u001b[39;49m\u001b[39m\"\u001b[39;49m, pages \u001b[39m=\u001b[39;49m \u001b[39m2000\u001b[39;49m)\n",
      "\u001b[1;32m/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb Cell 3\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m     pages \u001b[39m=\u001b[39m max_page\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# Start scraping pages\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m elements \u001b[39m=\u001b[39m scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# Pass the elements to the elements_and_ids function\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m elements_df \u001b[39m=\u001b[39m elements_and_ids(elements)\n",
      "\u001b[1;32m/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPage \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m = \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(\u001b[39m100\u001b[39m\u001b[39m*\u001b[39mi\u001b[39m/\u001b[39mpages, \u001b[39m2\u001b[39m)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39m progress. Estimated remaining time: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mround\u001b[39m(remaining_time_estimate, \u001b[39m2\u001b[39m)) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m seconds.\u001b[39m\u001b[39m\"\u001b[39m, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     new_url \u001b[39m=\u001b[39m url \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?strana=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     new_elements \u001b[39m=\u001b[39m get_page_elements(browser, new_url)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     elements\u001b[39m.\u001b[39mextend(new_elements)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m browser\u001b[39m.\u001b[39mquit()\n",
      "\u001b[1;32m/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load page\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m browser\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(random\u001b[39m.\u001b[39;49muniform(\u001b[39m1.0\u001b[39;49m, \u001b[39m1.5\u001b[39;49m))  \u001b[39m# Wait a bit for the page to load\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Get page HTML\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/patrik/Documents/GitHub/house-prices-prediction/get_urls.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m innerHTML \u001b[39m=\u001b[39m browser\u001b[39m.\u001b[39mexecute_script(\u001b[39m\"\u001b[39m\u001b[39mreturn document.body.innerHTML\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_soup_elements(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
