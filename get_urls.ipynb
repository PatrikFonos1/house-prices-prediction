{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_elements(browser, url):\n",
    "    \"\"\"Gets all href elements from a given url page using the browser webdriver\"\"\"\n",
    "    # Load page\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  # Wait a bit for the page to load\n",
    "    # Get page HTML\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML, 'lxml')\n",
    "    # Find all elements with the desired href attribute and add them to a list\n",
    "    elements = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^/detail/\")})]\n",
    "    return elements[::2]  # Return every other element\n",
    "\n",
    "def get_max_pages(soup):\n",
    "    \"\"\"Gets the maximum number of pages from a BeautifulSoup object\"\"\"\n",
    "    # Find the text showing the total number of records\n",
    "    records = soup.find_all(class_='numero ng-binding')[1].text\n",
    "    # Extract the number from the text\n",
    "    records = int(\"\".join(re.split(r'\\D', records)))\n",
    "    # Calculate the number of pages (20 records per page)\n",
    "    return math.ceil(records / 20)\n",
    "\n",
    "def print_scraping_info(typ_obchodu, typ_stavby, records, max_page, pages):\n",
    "    \"\"\"Prints some information about what is being scraped\"\"\"\n",
    "    print(\"----------------\")\n",
    "    print(\"Scraping: \" + typ_obchodu + \" \" + typ_stavby)\n",
    "    print(\"Total listings: \" + str(records))\n",
    "    print(\"Total pages: \" + str(max_page))\n",
    "    print(\"Scraping \" + str(pages) + \" pages.\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "def scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-minimized\") # Starts Chrome minimized\n",
    "    \"\"\"Scrape a given number of pages for href elements\"\"\"\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    # Get elements from the first page\n",
    "    elements = get_page_elements(browser, url)\n",
    "    # Print information about what we're scraping\n",
    "    print_scraping_info(typ_obchodu, typ_stavby, len(elements), max_page, pages)\n",
    "    # Scrape the remaining pages\n",
    "    for i in range(1, pages):\n",
    "        # Calculate elapsed time and remaining time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        time_per_page = elapsed_time / i\n",
    "        remaining_pages = pages - i\n",
    "        remaining_time_estimate = time_per_page * remaining_pages\n",
    "\n",
    "        print('\\r'+\"Page \" + str(i) + \" = \" + str(round(100*i/pages, 2)) + \"% progress. Estimated remaining time: \" + str(round(remaining_time_estimate, 2)) + \" seconds.\", end=\"\")\n",
    "\n",
    "        new_url = url + \"?strana=\" + str(i+1)\n",
    "        new_elements = get_page_elements(browser, new_url)\n",
    "        elements.extend(new_elements)\n",
    "    browser.quit()\n",
    "    return elements\n",
    "\n",
    "def get_id(url):\n",
    "    \"\"\"Extract the ID from a URL\"\"\"\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "def elements_and_ids(x):\n",
    "    \"\"\"Create a DataFrame of URLs and IDs, remove duplicates, and save to a CSV file\"\"\"\n",
    "    elements = pd.DataFrame({\"url\":x})\n",
    "    elements[\"url_id\"] = elements[\"url\"].apply(get_id)\n",
    "    \n",
    "    len_before = len(elements)\n",
    "    # Remove duplicates\n",
    "    elements.drop_duplicates(subset=[\"url\", \"url_id\"], keep=\"first\", inplace=True)\n",
    "    len_after = len(elements)\n",
    "    \n",
    "    print(f\"-- Removed {len_before - len_after} records due to duplication.\")\n",
    "    today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    filename = f'{today_date}_urls.csv'\n",
    "    elements.to_csv(filename, index=False)\n",
    "    return elements\n",
    "\n",
    "def get_soup_elements(typ_obchodu=\"prodej\", typ_stavby=\"byty\", pages=1):\n",
    "    \"\"\"Main function to get href elements for a given type of trade and construction\"\"\"\n",
    "    url = f\"https://www.sreality.cz/hledani/{typ_obchodu}/{typ_stavby}\"\n",
    "    browser = webdriver.Chrome()\n",
    "    # Get initial soup to extract maximum pages\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  \n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml')\n",
    "    max_page = get_max_pages(soup)\n",
    "    # If 2000 pages were requested, set pages to max_page\n",
    "    if pages == 2000:\n",
    "        pages = max_page\n",
    "    # Start scraping pages\n",
    "    elements = scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages)\n",
    "    # Pass the elements to the elements_and_ids function\n",
    "    elements_df = elements_and_ids(elements)\n",
    "    return elements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Scraping: prodej byty\n",
      "Total listings: 70\n",
      "Total pages: 974\n",
      "Scraping 974 pages.\n",
      "----------------\n",
      "Page 973 = 99.9% progress. Estimated remaining time: 3.43 seconds.....-- Removed 38554 records due to duplication.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>url_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/detail/prodej/byt/3+1/brno-lisen-strnadova/84...</td>\n",
       "      <td>847426892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/detail/prodej/byt/2+kk/libcice-nad-vltavou-li...</td>\n",
       "      <td>461526092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/detail/prodej/byt/3+kk/libcice-nad-vltavou-li...</td>\n",
       "      <td>1687311436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/detail/prodej/byt/3+kk/liberec-liberec-xxx-vr...</td>\n",
       "      <td>63538508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/detail/prodej/byt/4+kk/trhove-sviny-trhove-sv...</td>\n",
       "      <td>741217356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53976</th>\n",
       "      <td>/detail/prodej/byt/2+1/loket-loket-kostelni/23...</td>\n",
       "      <td>2397433436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53980</th>\n",
       "      <td>/detail/prodej/byt/2+1/karlovy-vary-karlovy-va...</td>\n",
       "      <td>400612956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53983</th>\n",
       "      <td>/detail/prodej/byt/3+kk/pisek--/4244561500</td>\n",
       "      <td>4244561500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53987</th>\n",
       "      <td>/detail/prodej/byt/2+kk/marianske-lazne-marian...</td>\n",
       "      <td>4023578204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53990</th>\n",
       "      <td>/detail/prodej/byt/atypicky/marianske-lazne-ma...</td>\n",
       "      <td>1825762908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15440 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url      url_id\n",
       "0      /detail/prodej/byt/3+1/brno-lisen-strnadova/84...   847426892\n",
       "4      /detail/prodej/byt/2+kk/libcice-nad-vltavou-li...   461526092\n",
       "7      /detail/prodej/byt/3+kk/libcice-nad-vltavou-li...  1687311436\n",
       "11     /detail/prodej/byt/3+kk/liberec-liberec-xxx-vr...    63538508\n",
       "14     /detail/prodej/byt/4+kk/trhove-sviny-trhove-sv...   741217356\n",
       "...                                                  ...         ...\n",
       "53976  /detail/prodej/byt/2+1/loket-loket-kostelni/23...  2397433436\n",
       "53980  /detail/prodej/byt/2+1/karlovy-vary-karlovy-va...   400612956\n",
       "53983         /detail/prodej/byt/3+kk/pisek--/4244561500  4244561500\n",
       "53987  /detail/prodej/byt/2+kk/marianske-lazne-marian...  4023578204\n",
       "53990  /detail/prodej/byt/atypicky/marianske-lazne-ma...  1825762908\n",
       "\n",
       "[15440 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_soup_elements(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
