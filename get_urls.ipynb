{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import timeit\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "import requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.13; rv:61.0) Gecko/20100101 Firefox/61.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.79 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:60.0) Gecko/20100101 Firefox/60.0'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_elements(browser, url):\n",
    "    \"\"\"Gets all href elements from a given url page using the browser webdriver\"\"\"\n",
    "    # Load page\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  # Wait a bit for the page to load\n",
    "    # Get page HTML\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML, 'lxml')\n",
    "    # Find all elements with the desired href attribute and add them to a list\n",
    "    elements = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^/detail/\")})]\n",
    "    return elements[::2]  # Return every other element\n",
    "\n",
    "def get_max_pages(soup):\n",
    "    \"\"\"Gets the maximum number of pages from a BeautifulSoup object\"\"\"\n",
    "    # Find the text showing the total number of records\n",
    "    records = soup.find_all(class_='numero ng-binding')[1].text\n",
    "    # Extract the number from the text\n",
    "    records = int(\"\".join(re.split(r'\\D', records)))\n",
    "    # Calculate the number of pages (20 records per page)\n",
    "    return math.ceil(records / 20)\n",
    "\n",
    "def print_scraping_info(typ_obchodu, typ_stavby, records, max_page, pages):\n",
    "    \"\"\"Prints some information about what is being scraped\"\"\"\n",
    "    print(\"----------------\")\n",
    "    print(\"Scraping: \" + typ_obchodu + \" \" + typ_stavby)\n",
    "    print(\"Total listings: \" + str(records))\n",
    "    print(\"Total pages: \" + str(max_page))\n",
    "    print(\"Scraping (only) \" + str(pages) + \" pages.\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "def scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--start-minimized\") # Starts Chrome minimized\n",
    "    \"\"\"Scrape a given number of pages for href elements\"\"\"\n",
    "    browser = webdriver.Chrome(options=chrome_options)\n",
    "    # Get elements from the first page\n",
    "    elements = get_page_elements(browser, url)\n",
    "    # Print information about what we're scraping\n",
    "    print_scraping_info(typ_obchodu, typ_stavby, len(elements), max_page, pages)\n",
    "    # Scrape the remaining pages\n",
    "    for i in range(1, pages):\n",
    "        print('\\r'+\"Page \" + str(i) + \" = \" + str(round(100*i/pages, 2)) + \"% progress. Remaining time: \" + str(round(random.uniform(3.4, 3.8)*(pages-i), 2 )) + \" seconds.\", end=\"\")\n",
    "        new_url = url + \"?strana=\" + str(i+1)\n",
    "        new_elements = get_page_elements(browser, new_url)\n",
    "        elements.extend(new_elements)\n",
    "    browser.quit()\n",
    "    return elements\n",
    "\n",
    "def get_id(url):\n",
    "    \"\"\"Extract the ID from a URL\"\"\"\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "def elements_and_ids(x):\n",
    "    \"\"\"Create a DataFrame of URLs and IDs, remove duplicates, and save to a CSV file\"\"\"\n",
    "    elements = pd.DataFrame({\"url\":x})\n",
    "    elements[\"url_id\"] = elements[\"url\"].apply(get_id)\n",
    "    \n",
    "    len_before = len(elements)\n",
    "    # Remove duplicates\n",
    "    elements.drop_duplicates(subset=[\"url\", \"url_id\"], keep=\"first\", inplace=True)\n",
    "    len_after = len(elements)\n",
    "    \n",
    "    print(f\"-- Removed {len_before - len_after} records due to duplication.\")\n",
    "    today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    filename = f'{today_date}_urls.csv'\n",
    "    elements.to_csv(filename, index=False)\n",
    "    return elements\n",
    "\n",
    "def get_soup_elements(typ_obchodu=\"prodej\", typ_stavby=\"byty\", pages=1):\n",
    "    \"\"\"Main function to get href elements for a given type of trade and construction\"\"\"\n",
    "    url = f\"https://www.sreality.cz/hledani/{typ_obchodu}/{typ_stavby}\"\n",
    "    browser = webdriver.Chrome()\n",
    "    # Get initial soup to extract maximum pages\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  \n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml')\n",
    "    max_page = get_max_pages(soup)\n",
    "    # If 2000 pages were requested, set pages to max_page\n",
    "    if pages == 2000:\n",
    "        pages = max_page\n",
    "    # Start scraping pages\n",
    "    elements = scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages)\n",
    "    # Pass the elements to the elements_and_ids function\n",
    "    elements_df = elements_and_ids(elements)\n",
    "    return elements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_soup_elements(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
