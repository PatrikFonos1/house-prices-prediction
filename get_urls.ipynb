{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_elements(browser, url):\n",
    "    \"\"\"Gets all href elements from a given url page using the browser webdriver\"\"\"\n",
    "    # Load page\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  # Wait a bit for the page to load\n",
    "    # Get page HTML\n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML, 'lxml')\n",
    "    # Find all elements with the desired href attribute and add them to a list\n",
    "    elements = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^/detail/\")})]\n",
    "    return elements[::2]  # Return every other element\n",
    "\n",
    "def get_max_pages(soup):\n",
    "    \"\"\"Gets the maximum number of pages from a BeautifulSoup object\"\"\"\n",
    "    # Find the text showing the total number of records\n",
    "    records = soup.find_all(class_='numero ng-binding')[1].text\n",
    "    # Extract the number from the text\n",
    "    records = int(\"\".join(re.split(r'\\D', records)))\n",
    "    # Calculate the number of pages (20 records per page)\n",
    "    return math.ceil(records / 20)\n",
    "\n",
    "def print_scraping_info(typ_obchodu, typ_stavby, records, max_page, pages):\n",
    "    \"\"\"Prints some information about what is being scraped\"\"\"\n",
    "    print(\"----------------\")\n",
    "    print(\"Scraping: \" + typ_obchodu + \" \" + typ_stavby)\n",
    "    print(\"Total listings: \" + str(records))\n",
    "    print(\"Total pages: \" + str(max_page))\n",
    "    print(\"Scraping (only) \" + str(pages) + \" pages.\")\n",
    "    print(\"----------------\")\n",
    "\n",
    "def scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages):\n",
    "    \"\"\"Scrape a given number of pages for href elements\"\"\"\n",
    "    browser = webdriver.Chrome()\n",
    "    # Get elements from the first page\n",
    "    elements = get_page_elements(browser, url)\n",
    "    # Print information about what we're scraping\n",
    "    print_scraping_info(typ_obchodu, typ_stavby, len(elements), max_page, pages)\n",
    "    # Scrape the remaining pages\n",
    "    for i in range(1, pages):\n",
    "        print('\\r'+\"Page \" + str(i) + \" = \" + str(round(100*i/pages, 2)) + \"% progress. Remaining time: \" + str(round(random.uniform(3.4, 3.8)*(pages-i), 2 )) + \" seconds.\", end=\"\")\n",
    "        new_url = url + \"?strana=\" + str(i+1)\n",
    "        new_elements = get_page_elements(browser, new_url)\n",
    "        elements.extend(new_elements)\n",
    "    browser.quit()\n",
    "    return elements\n",
    "\n",
    "def get_id(url):\n",
    "    \"\"\"Extract the ID from a URL\"\"\"\n",
    "    return url.split(\"/\")[-1]\n",
    "\n",
    "def elements_and_ids(x):\n",
    "    \"\"\"Create a DataFrame of URLs and IDs, remove duplicates, and save to a CSV file\"\"\"\n",
    "    elements = pd.DataFrame({\"url\":x})\n",
    "    elements[\"url_id\"] = elements[\"url\"].apply(get_id)\n",
    "    \n",
    "    len_before = len(elements)\n",
    "    # Remove duplicates\n",
    "    elements.drop_duplicates(subset=[\"url\", \"url_id\"], keep=\"first\", inplace=True)\n",
    "    len_after = len(elements)\n",
    "    \n",
    "    print(f\"-- Removed {len_before - len_after} records due to duplication.\")\n",
    "    today_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    filename = f'{today_date}_urls.csv'\n",
    "    elements.to_csv(filename, index=False)\n",
    "    return elements\n",
    "\n",
    "def get_soup_elements(typ_obchodu=\"prodej\", typ_stavby=\"byty\", pages=1):\n",
    "    \"\"\"Main function to get href elements for a given type of trade and construction\"\"\"\n",
    "    url = f\"https://www.sreality.cz/hledani/{typ_obchodu}/{typ_stavby}\"\n",
    "    browser = webdriver.Chrome()\n",
    "    # Get initial soup to extract maximum pages\n",
    "    browser.get(url)\n",
    "    time.sleep(random.uniform(1.0, 1.5))  \n",
    "    innerHTML = browser.execute_script(\"return document.body.innerHTML\")\n",
    "    soup = BeautifulSoup(innerHTML,'lxml')\n",
    "    max_page = get_max_pages(soup)\n",
    "    # If 2000 pages were requested, set pages to max_page\n",
    "    if pages == 2000:\n",
    "        pages = max_page\n",
    "    # Start scraping pages\n",
    "    elements = scrape_pages(typ_obchodu, typ_stavby, max_page, url, pages)\n",
    "    # Pass the elements to the elements_and_ids function\n",
    "    elements_df = elements_and_ids(elements)\n",
    "    return elements_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "Scraping: prodej byty\n",
      "Total listings: 70\n",
      "Total pages: 1050\n",
      "Scraping (only) 1050 pages.\n",
      "----------------\n",
      "Page 1049 = 99.9% progress. Remaining time: 3.46 seconds....-- Removed 47216 records due to duplication.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>url_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/detail/prodej/byt/2+kk/praha-michle-tymlova/7...</td>\n",
       "      <td>722035788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/detail/prodej/byt/3+kk/praha-liben-na-sypkem/...</td>\n",
       "      <td>1669567564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/detail/prodej/byt/1+kk/destne-v-orlickych-hor...</td>\n",
       "      <td>2418832972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/detail/prodej/byt/1+1/jablonec-nad-nisou-jabl...</td>\n",
       "      <td>3930260556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/detail/prodej/byt/3+kk/praha-praha-1-narodni/...</td>\n",
       "      <td>4078400588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66311</th>\n",
       "      <td>/detail/prodej/byt/4+1/praha-veleslavin-krenov...</td>\n",
       "      <td>2493924940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66314</th>\n",
       "      <td>/detail/prodej/byt/3+kk/novy-bydzov--/2597877324</td>\n",
       "      <td>2597877324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66318</th>\n",
       "      <td>/detail/prodej/byt/2+1/praha-strasnice-oravska...</td>\n",
       "      <td>1551496780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66321</th>\n",
       "      <td>/detail/prodej/byt/2+1/moravsky-beroun--/38518...</td>\n",
       "      <td>3851892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66325</th>\n",
       "      <td>/detail/prodej/byt/4+kk/praha-liben-primatorsk...</td>\n",
       "      <td>2431822172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19112 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url      url_id\n",
       "0      /detail/prodej/byt/2+kk/praha-michle-tymlova/7...   722035788\n",
       "3      /detail/prodej/byt/3+kk/praha-liben-na-sypkem/...  1669567564\n",
       "7      /detail/prodej/byt/1+kk/destne-v-orlickych-hor...  2418832972\n",
       "10     /detail/prodej/byt/1+1/jablonec-nad-nisou-jabl...  3930260556\n",
       "14     /detail/prodej/byt/3+kk/praha-praha-1-narodni/...  4078400588\n",
       "...                                                  ...         ...\n",
       "66311  /detail/prodej/byt/4+1/praha-veleslavin-krenov...  2493924940\n",
       "66314   /detail/prodej/byt/3+kk/novy-bydzov--/2597877324  2597877324\n",
       "66318  /detail/prodej/byt/2+1/praha-strasnice-oravska...  1551496780\n",
       "66321  /detail/prodej/byt/2+1/moravsky-beroun--/38518...  3851892300\n",
       "66325  /detail/prodej/byt/4+kk/praha-liben-primatorsk...  2431822172\n",
       "\n",
       "[19112 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_soup_elements(typ_obchodu = \"prodej\", typ_stavby = \"byty\", pages = 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
